{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CAR DETECTION MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "from tkinter import messagebox\n",
    "from tkinter import Label\n",
    "from PIL import Image, ImageTk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to load and display images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_images_from_directory(directory):\n",
    "    files = os.listdir(directory)\n",
    "    for file in files:\n",
    "        file_path = os.path.join(directory, file)\n",
    "        image = cv2.imread(file_path)\n",
    "        if image is not None:\n",
    "            cv2.imshow(\"Image\", image)\n",
    "            key = cv2.waitKey(0)\n",
    "            if key == ord('q'):\n",
    "                print(\"Quitting...\")\n",
    "                break\n",
    "        else:\n",
    "            print(f\"Could not load image: {file_path}\")\n",
    "\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quitting...\n"
     ]
    }
   ],
   "source": [
    "display_images_from_directory(\"Traffic Images/training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quitting...\n"
     ]
    }
   ],
   "source": [
    "display_images_from_directory(\"Traffic Images/testing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to training and testing directories\n",
    "train_dir = \"Traffic Images/training\"\n",
    "test_dir = \"Traffic Images/testing\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load the Haar Cascade for car detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "car_cascade_path = 'cars.xml'  \n",
    "car_cascade = cv2.CascadeClassifier(car_cascade_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the COCO class labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_path = \"yolov3.weights\" \n",
    "config_path = \"yolov3.cfg\"\n",
    "labels_path = \"coco.names\"   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['person', 'bicycle', 'car', 'motorbike', 'aeroplane', 'bus', 'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'sofa', 'pottedplant', 'bed', 'diningtable', 'toilet', 'tvmonitor', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush']\n"
     ]
    }
   ],
   "source": [
    "with open(labels_path, 'r') as f:\n",
    "    labels = f.read().strip().split(\"\\n\")\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load YOLOv3 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = cv2.dnn.readNetFromDarknet(config_path, weights_path)\n",
    "layer_names = net.getLayerNames()\n",
    "output_layers = [layer_names[i - 1] for i in net.getUnconnectedOutLayers()]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the UTKFace dataset for Gender classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "UTK = \"UTKFace\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "labels = []\n",
    "\n",
    "for img_name in os.listdir(UTK):\n",
    "    img_path = os.path.join(UTK, img_name)\n",
    "    img = cv2.imread(img_path)\n",
    "    img = cv2.resize(img, (64, 64))\n",
    "    label = int(img_name.split(\"_\")[1])  # Gender label\n",
    "    data.append(img)\n",
    "    labels.append(label)\n",
    "\n",
    "data = np.array(data, dtype=\"float\") / 255.0\n",
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "(trainX, testX, trainY, testY) = train_test_split(data, labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SARATHLAL\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Build the model\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), padding=\"same\", activation=\"relu\", input_shape=(64, 64, 3)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(64, (3, 3), padding=\"same\", activation=\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation=\"relu\"))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation=\"sigmoid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=Adam(0.001), metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m593/593\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 49ms/step - accuracy: 0.7319 - loss: 0.5326 - val_accuracy: 0.8577 - val_loss: 0.3198\n",
      "Epoch 2/10\n",
      "\u001b[1m593/593\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 48ms/step - accuracy: 0.8569 - loss: 0.3196 - val_accuracy: 0.8804 - val_loss: 0.2804\n",
      "Epoch 3/10\n",
      "\u001b[1m593/593\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 48ms/step - accuracy: 0.8799 - loss: 0.2794 - val_accuracy: 0.8827 - val_loss: 0.2674\n",
      "Epoch 4/10\n",
      "\u001b[1m593/593\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 49ms/step - accuracy: 0.8929 - loss: 0.2500 - val_accuracy: 0.8855 - val_loss: 0.2540\n",
      "Epoch 5/10\n",
      "\u001b[1m593/593\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 49ms/step - accuracy: 0.9000 - loss: 0.2354 - val_accuracy: 0.8946 - val_loss: 0.2503\n",
      "Epoch 6/10\n",
      "\u001b[1m593/593\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 50ms/step - accuracy: 0.9077 - loss: 0.2111 - val_accuracy: 0.8914 - val_loss: 0.2569\n",
      "Epoch 7/10\n",
      "\u001b[1m593/593\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 50ms/step - accuracy: 0.9158 - loss: 0.2028 - val_accuracy: 0.8954 - val_loss: 0.2486\n",
      "Epoch 8/10\n",
      "\u001b[1m593/593\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 49ms/step - accuracy: 0.9192 - loss: 0.1886 - val_accuracy: 0.8956 - val_loss: 0.2677\n",
      "Epoch 9/10\n",
      "\u001b[1m593/593\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 49ms/step - accuracy: 0.9265 - loss: 0.1827 - val_accuracy: 0.9032 - val_loss: 0.2502\n",
      "Epoch 10/10\n",
      "\u001b[1m593/593\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 49ms/step - accuracy: 0.9362 - loss: 0.1567 - val_accuracy: 0.8979 - val_loss: 0.2571\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1c4037cd510>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "model.fit(trainX, trainY, validation_data=(testX, testY), batch_size=32, epochs=10, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "# Save the model\n",
    "model.save('gender_classification_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "# Load the gender classification model\n",
    "gender_model = load_model('gender_classification_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to detect objects in an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_objects(image):\n",
    "    height, width, channels = image.shape\n",
    "    blob = cv2.dnn.blobFromImage(image, 0.00392, (416, 416), (0, 0, 0), True, crop=False)\n",
    "    net.setInput(blob)\n",
    "    outs = net.forward(output_layers)\n",
    "    class_ids = []\n",
    "    confidences = []\n",
    "    boxes = []\n",
    "    for out in outs:\n",
    "        for detection in out:\n",
    "            scores = detection[5:]\n",
    "            class_id = np.argmax(scores)\n",
    "            confidence = scores[class_id]\n",
    "            if confidence > 0.5:  # Filter out weak detections\n",
    "                center_x = int(detection[0] * width)\n",
    "                center_y = int(detection[1] * height)\n",
    "                w = int(detection[2] * width)\n",
    "                h = int(detection[3] * height)\n",
    "                x = int(center_x - w / 2)\n",
    "                y = int(center_y - h / 2)\n",
    "                boxes.append([x, y, w, h])\n",
    "                confidences.append(float(confidence))\n",
    "                class_ids.append(class_id)\n",
    "    indexes = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)\n",
    "    filtered_boxes = [boxes[i] for i in range(len(boxes)) if i in indexes]\n",
    "    filtered_classes = [class_ids[i] for i in range(len(class_ids)) if i in indexes]\n",
    "    return filtered_boxes, filtered_classes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to detect cars using Haar Cascade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_cars(image):\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    cars = car_cascade.detectMultiScale(gray, 1.1, 1)\n",
    "    car_boxes = [[x, y, w, h] for (x, y, w, h) in cars]\n",
    "    return car_boxes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to classify car colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_car_color(image, boxes):\n",
    "    colors = []\n",
    "    for box in boxes:\n",
    "        x, y, w, h = box\n",
    "        car_image = image[y:y+h, x:x+w]\n",
    "        avg_color = np.mean(car_image, axis=(0, 1))\n",
    "        if avg_color[2] > avg_color[0]:\n",
    "            colors.append('blue')\n",
    "        else:\n",
    "            colors.append('red')\n",
    "    return colors\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to predict gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_gender(image, boxes):\n",
    "    males, females = 0, 0\n",
    "    for box in boxes:\n",
    "        x, y, w, h = box\n",
    "        person_image = image[y:y+h, x:x+w]\n",
    "        person_image = cv2.resize(person_image, (64, 64)) / 255.0\n",
    "        person_image = np.expand_dims(person_image, axis=0)\n",
    "        gender = gender_model.predict(person_image)\n",
    "        if gender[0][0] > 0.5:\n",
    "            females += 1\n",
    "        else:\n",
    "            males += 1\n",
    "    return males, females"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to process the traffic image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_traffic_image(image):\n",
    "    boxes, classes = detect_objects(image)\n",
    "    car_boxes = detect_cars(image)\n",
    "    person_boxes = [box for box, cls in zip(boxes, classes) if cls == 0]  # Class 0 for people\n",
    "\n",
    "    # Swap car colors\n",
    "    car_colors = classify_car_color(image, car_boxes)\n",
    "    swapped_colors = ['red' if color == 'blue' else 'blue' for color in car_colors]\n",
    "\n",
    "    # Predict gender for people\n",
    "    males, females = predict_gender(image, person_boxes)\n",
    "\n",
    "    # Count other vehicles\n",
    "    other_vehicle_count = sum(1 for cls in classes if cls not in [0, 2])\n",
    "\n",
    "    return swapped_colors, len(car_boxes), males, females, other_vehicle_count, car_boxes + person_boxes, ['car'] * len(car_boxes) + ['person'] * len(person_boxes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to display the detection results on the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_detections(image, boxes, classes):\n",
    "    for i, box in enumerate(boxes):\n",
    "        x, y, w, h = box\n",
    "        label = classes[i]\n",
    "        color = (0, 255, 0)  # Green color for rectangles\n",
    "        cv2.rectangle(image, (x, y), (x + w, y + h), color, 2)\n",
    "        cv2.putText(image, label, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "    return image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to open and process a single image file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_image():\n",
    "    file_path = filedialog.askopenfilename()\n",
    "    if file_path:\n",
    "        image = cv2.imread(file_path)\n",
    "        if image is not None:\n",
    "            results = process_traffic_image(image)\n",
    "            image_with_detections = draw_detections(image, results[5], results[6])\n",
    "\n",
    "            # Convert the image to RGB and display in the GUI\n",
    "            image_rgb = cv2.cvtColor(image_with_detections, cv2.COLOR_BGR2RGB)\n",
    "            im_pil = Image.fromarray(image_rgb)\n",
    "            imgtk = ImageTk.PhotoImage(image=im_pil)\n",
    "\n",
    "            img_label.config(image=imgtk)\n",
    "            img_label.image = imgtk\n",
    "\n",
    "            result_text = f\"Swapped Colors: {results[0]}\\n\" \\\n",
    "                          f\"Number of Cars: {results[1]}\\n\" \\\n",
    "                          f\"Number of Males: {results[2]}\\n\" \\\n",
    "                          f\"Number of Females: {results[3]}\\n\" \\\n",
    "                          f\"Other Vehicles: {results[4]}\"\n",
    "            result_label.config(text=result_text)\n",
    "\n",
    "        else:\n",
    "            messagebox.showerror(\"Error\", f\"Could not load image: {file_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up the GUI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = tk.Tk()\n",
    "root.title(\"Traffic Signal Analysis\")\n",
    "\n",
    "btn_open_image = tk.Button(root, text=\"Upload Image\", command=open_image)\n",
    "btn_open_image.pack(pady=20)\n",
    "\n",
    "img_label = Label(root)\n",
    "img_label.pack(pady=10)\n",
    "\n",
    "result_label = Label(root, text=\"\", justify=tk.LEFT)\n",
    "result_label.pack(pady=10)\n",
    "\n",
    "root.mainloop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
